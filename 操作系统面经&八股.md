# 操作系统面经&八股

------

## 进程、线程、协程区别与联系？

进程、线程和协程是计算机程序执行的三个不同层次。

**进程（Process）**： 进程是操作系统进行资源分配和调度的基本单位，是一个独立运行的程序实体。每个进程拥有独立的内存空间、文件描述符、寄存器状态等资源。进程之间的资源是相互隔离的，因此进程间通信需要通过操作系统提供的特定机制（如管道、消息队列、共享内存等）进行。由于进程拥有独立的资源，所以进程间的切换和调度开销较大。

**线程（Thread）**： 线程是操作系统调度执行的最小单位，是进程内的一个执行流。一个进程可以拥有多个线程，这些线程共享进程的资源（如内存空间、文件描述符等）。由于线程共享相同的资源，线程间通信相对简单，可以直接通过共享变量、锁等方式进行。线程相较于进程，上下文切换和调度开销较小。但多个线程并发执行时，需要处理好同步和互斥问题，以避免数据不一致或竞争条件。

**协程（Coroutine**）： 协程是一种用户态的轻量级线程，它的调度和切换完全由程序控制，不依赖于操作系统的调度。协程之间共享线程的资源，因此协程间通信也可以通过共享变量、锁等方式进行。协程的优势在于能够轻松地实现高并发，因为协程切换和调度的开销非常小。协程适用于I/O密集型任务，通过异步I/O可以有效地提高程序的性能。

**联系**

- 线程属于进程，多个线程共享进程的资源。一个进程可以包含多个线程，这些线程共同完成任务，提高程序的并发性。
- 协程属于线程，多个协程共享线程的资源。一个线程可以包含多个协程，这些协程协同完成任务，提高程序的性能。
- 进程、线程和协程在执行程序时，都需要面对同步、互斥和通信等问题。在实际应用中，可以根据需求和场景选择合适的执行实体来实现最优的性能和资源利用。



## **线程和进程的区别**

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/J0g14CUwaZcDoMACjnyRRutZlx3uQaxMfBUqUOQsWtCuD3ibR3rPGwQjHZZLttWz8CibLnv1eCfZGhjWrIT0pQoQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- **本质区别**：进程是操作系统资源分配的基本单位，而线程是任务调度和执行的基本单位

- **包含关系**： 一个进程可以包含多个线程

- **在开销方面**：每个进程都有独立的代码和数据空间（程序上下文），由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O 设备等，所以进程之间的切换会有较大的开销；线程可以看做轻量级的进程，同一进程的线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器（PC），切换时只需保存和设置少量寄存器内容，所以开销小

- **内存分配方面**：系统在运行的时候会为每个进程分配不同的内存空间；而对线程而言，除了CPU外，系统不会为线程分配内存（线程所使用的资源来自其所属进程的资源），线程组之间只能共享资源。

  

### 进程的共享内存与线程的共享内存

进程的共享内存允许两个或多个进程共享一个给定的内存区，一个进程写⼊的东西，其他进程⻢上就能看到。

共享内存是最快的进程间通信方式，它是针对其他进程间通信方式运行效率低而专门设计的。

![图片](https://mmbiz.qpic.cn/mmbiz_png/z40lCFUAHpmWUdtbfia3licZqpTSXHgCf6f62g6Avz9QgLiciabNTNvwle2TwIHh6gRsWXDpkibUKey3oTm9p0xarhQ/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

缺点：当多进程竞争同一个共享资源时，会造成数据错乱的问题。

线程之间想要进行通信，可以通过消息传递和共享内存两种方法来完成。那 Java 采用的是共享内存的并发模型。

这个模型被称为 Java 内存模型，也就是 JMM，JMM 决定了一个线程对共享变量的写入何时对另外一个线程可见。

线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了共享变量的副本。当然了，本地内存是 JMM 的一个抽象概念，并不真实存在。

![图片](https://mmbiz.qpic.cn/mmbiz_png/z40lCFUAHpmWUdtbfia3licZqpTSXHgCf6zf0Ao5DR5CvYRcWP0LY515GIGmKxicmrkRjE2fQXRK4IlC92wOem25w/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

线程 A 与线程 B 之间如要通信的话，必须要经历下面 2 个步骤：

- 线程 A 把本地内存 A 中的共享变量副本刷新到主内存中。
- 线程 B 到主内存中读取线程 A 刷新过的共享变量，再同步到自己的共享变量副本中。

![图片](https://mmbiz.qpic.cn/mmbiz_png/z40lCFUAHpmWUdtbfia3licZqpTSXHgCf67ciaytRz1v1IrSE398mvyHIC09ubp69nXxFibiaQ5tj4vCtY3rRYauxrw/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)



### 协程为什么比线程快？

协程比线程快的主要原因有以下几点：

- **用户态切换**：协程是在用户态下进行切换，不涉及内核态的上下文切换和系统调用，切换成本低，执行效率高。
- **轻量级**：协程是由用户自己管理的，不需要操作系统进行调度和管理，占用的资源较少，创建和销毁的开销小。
- **高并发**：协程可以在同一个线程内并发执行，避免了线程切换和同步的开销，提高了并发处理能力。



## 讲一讲用户线程与内核线程？

**用户线程**： 用户线程是完全在用户空间中实现和管理的线程。它们的创建、同步和调度都由用户级别的线程库（如POSIX线程库，即Pthreads）处理，而不需要内核直接参与。由于用户线程的操作不涉及系统调用，它们的创建和切换开销相对较小。用户线程的一个主要限制是，它们不能充分利用多核处理器的并行能力。因为操作系统调度的基本单位是内核线程，当一个用户线程阻塞时（如I/O操作），整个进程都会被阻塞，即使其他用户线程仍处于就绪状态。这可能导致多处理器系统中的性能下降。

**内核线程**： 内核线程是由操作系统内核直接支持和管理的线程。内核负责创建、调度和销毁内核线程，每个内核线程都拥有独立的内核栈和线程上下文。由于内核线程是操作系统调度的基本单位，它们可以充分利用多处理器系统的并行能力。内核线程的缺点是，它们的创建、切换和同步操作涉及系统调用，导致较大的开销。此外，内核线程需要更多的内核资源（如内核栈），这可能在大量线程的情况下导致资源耗尽。

**总结**： 用户线程和内核线程分别代表了两种线程实现方式，用户线程的开销较小，但在多处理器系统中可能无法充分利用并行能力；内核线程可以充分利用多处理器的并行能力，但开销较大。在实际应用中，可以根据具体需求和性能要求选择合适的线程类型。有些操作系统（如Linux、Windows）采用了一种混合模型，将用户线程和内核线程结合起来。在这种模型中，每个用户线程都映射到一个内核线程上，这样可以同时利用用户线程的轻量级特性和内核线程的并行能力。

![image-20240914235214923](assets\image-20240914235214923.png)

![image-20240914235231770](assets\image-20240914235231770.png)

![image-20240914235255654](assets\image-20240914235255654.png)

**参考文章：**[**用户态和内核态：用户态线程和内核态线程有什么区别？_线程内核态和用户态-CSDN博客**](https://blog.csdn.net/qq_41489540/article/details/109261692?ops_request_misc=%7B%22request%5Fid%22%3A%22DE282B19-0D39-46A4-9378-AF33303DB234%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=DE282B19-0D39-46A4-9378-AF33303DB234&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-8-109261692-null-null.142^v100^pc_search_result_base2&utm_term=用户态线程和内核态线程&spm=1018.2226.3001.4187)





## **为什么进程崩溃不会对其他进程产生很大影响**

主要是因为：

- **进程隔离性**：每个进程都有自己独立的内存空间，当一个进程崩溃时，其内存空间会被操作系统回收，不会影响其他进程的内存空间。这种进程间的隔离性保证了一个进程崩溃不会直接影响其他进程的执行。
- **进程独立性**：每个进程都是独立运行的，它们之间不会共享资源，如文件、网络连接等。因此，一个进程的崩溃通常不会对其他进程的资源产生影响。



## 进程间通信有哪些？

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/3-%E6%8F%90%E7%BA%B2.jpg)

进程间通信的方式有多种，包括：

1. **匿名管道（Pipe）**：单向通信，适用于有亲缘关系的进程（父子进程）之间通信。
   - 优点：简单易用，适用于单向通信。
   - 缺点：只能在有亲缘关系的进程之间使用，且只能实现单向通信。
2. **命名管道（Named Pipe）**：允许无亲缘关系的进程间进行通信。
   - 优点：允许无亲缘关系的进程进行通信。
   - 缺点：只能实现单向通信。
3. **消息队列（Message Queue）**：实现进程间的异步通信，通过消息缓冲区传递数据。
   - 优点：支持异步通信，可以实现多对多通信。
   - 缺点：复杂度较高，需要处理消息的发送和接收。
4. **共享内存（Shared Memory）**：多个进程共享同一块内存区域，实现高效的数据交换。
   - 优点：高效，适合大数据量的共享。
   - 缺点：需要处理同步和互斥问题，可能引起数据一致性和安全性问题。
5. **信号量（Semaphore）**：用于进程间的同步和互斥控制。
   - 优点：可以实现进程间的同步和互斥。
   - 缺点：复杂度较高，需要处理信号量的管理。
6. **套接字（Socket）**：适用于不同主机间的进程通信，可实现网络通信。
   - 优点：跨主机通信，支持多种通信协议。
   - 缺点：相比于其他方式，套接字通信开销较大。

> **补充：**
>
> **匿名管道**顾名思义，它没有名字标识，匿名管道是特殊文件只存在于内存，没有存在于文件系统中，shell 命令中的「|」竖线就是匿名管道，通信的数据是**无格式的流并且大小受限**，通信的方式是**单向**的，数据只能在一个方向上流动，如果要双向通信，需要创建两个管道，再来**匿名管道是只能用于存在父子关系的进程间通信**，匿名管道的生命周期随着进程创建而建立，随着进程终止而消失。
>
> **命名管道**突破了匿名管道只能在亲缘关系进程间的通信限制，因为使用命名管道的前提，需要在文件系统创建一个类型为 p 的设备文件，那么毫无关系的进程就可以通过这个设备文件进行通信。另外，不管是匿名管道还是命名管道，进程写入的数据都是**缓存在内核**中，另一个进程读取数据时候自然也是从内核中获取，同时通信数据都遵循**先进先出**原则，不支持 lseek 之类的文件定位操作。
>
> **消息队列**克服了管道通信的数据是无格式的字节流的问题，消息队列实际上是保存在内核的「消息链表」，消息队列的消息体是可以用户自定义的数据类型，发送数据时，会被分成一个一个独立的消息体，当然接收数据时，也要与发送方发送的消息体的数据类型保持一致，这样才能保证读取的数据是正确的。消息队列通信的速度不是最及时的，毕竟**每次数据的写入和读取都需要经过用户态与内核态之间的拷贝过程。**
>
> **共享内存**可以解决消息队列通信中用户态与内核态之间数据拷贝过程带来的开销，**它直接分配一个共享空间，每个进程都可以直接访问**，就像访问进程自己的空间一样快捷方便，不需要陷入内核态或者系统调用，大大提高了通信的速度，享有**最快**的进程间通信方式之名。但是便捷高效的共享内存通信，**带来新的问题，多进程竞争同个共享资源会造成数据的错乱。**
>
> 那么，就需要**信号量**来保护共享资源，以确保任何时刻只能有一个进程访问共享资源，这种方式就是互斥访问。**信号量不仅可以实现访问的互斥性，还可以实现进程间的同步**，信号量其实是一个计数器，表示的是资源个数，其值可以通过两个原子操作来控制，分别是 **P 操作和 V 操作**。
>
> 与信号量名字很相似的叫**信号**，它俩名字虽然相似，但功能一点儿都不一样。信号是**异步通信机制**，信号可以在应用进程和内核之间直接交互，内核也可以利用信号来通知用户空间的进程发生了哪些系统事件，信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令），一旦有信号发生，**进程有三种方式响应信号 1. 执行默认操作、2. 捕捉信号、3. 忽略信号**。有两个信号是应用进程无法捕捉和忽略的，即 SIGKILL 和 SIGSTOP，这是为了方便我们能在任何时候结束或停止某个进程。
>
> 前面说到的通信机制，都是工作于同一台主机，如果**要与不同主机的进程间通信，那么就需要 Socket 通信了**。Socket 实际上不仅用于不同的主机进程间通信，还可以用于本地主机进程间通信，可根据创建 Socket 的类型不同，分为三种常见的通信方式，一个是基于 TCP 协议的通信方式，一个是基于 UDP 协议的通信方式，一个是本地进程间通信方式。



## 线程间通信知道哪些？

> 同个进程下的线程之间都是共享进程的资源，只要是共享变量都可以做到线程间通信，比如全局变量，所以对于线程间关注的不是通信方式，而是关注多线程竞争共享资源的问题，信号量也同样可以在线程间实现互斥与同步：
>
> - 互斥的方式，可保证任意时刻只有一个线程访问共享资源；
> - 同步的方式，可保证线程 A 应在线程 B 之前执行；

在Linux系统中，线程间通信的方式包括：

- **互斥锁（Mutex）**：线程可以使用互斥锁来保护共享资源，确保同时只有一个线程可以访问该资源。
- **条件变量**：线程可以使用条件变量来等待特定条件的发生，以实现线程间的协调和通知。
- **信号量**：线程可以使用信号量来控制对共享资源的访问，实现线程间的同步和互斥。
- **读写锁**：允许多个线程同时读取共享资源，但只允许一个线程写入共享资源。



线程同步是两个或多个共享关键资源的线程的并发执行。应该同步线程以避免关键的资源使用冲突。

下面是几种常见的线程同步的方式：

1. **互斥锁(Mutex)** ：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。比如 Java 中的 `synchronized` 关键词和各种 `Lock` 都是这种机制。
2. **读写锁（Read-Write Lock）** ：允许多个线程同时读取共享资源，但只有一个线程可以对共享资源进行写操作。
3. **信号量(Semaphore)** ：它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量。
4. **屏障（Barrier）** ：屏障是一种同步原语，用于等待多个线程到达某个点再一起继续执行。当一个线程到达屏障时，它会停止执行并等待其他线程到达屏障，直到所有线程都到达屏障后，它们才会一起继续执行。比如 Java 中的 `CyclicBarrier` 是这种机制。
5. **事件(Event)** :Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操作。



## 线程切换详细过程是怎么样的？上下文保存在哪里？

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/J0g14CUwaZfQRicpOgAh6Vj9bLDxk52h8UesW5kgCia3jRQfUnXKOUy8APF3ELv1aSD7dCQEXmOK5icmgHuJPXO4w/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)



线程切换的详细过程可以分为以下几个步骤：

- 上下文保存：当操作系统决定切换到另一个线程时，它首先会保存当前线程的上下文信息。上下文信息包括寄存器状态、程序计数器、堆栈指针等，用于保存线程的执行状态。
- 切换到调度器：操作系统将执行权切换到调度器（Scheduler）。调度器负责选择下一个要执行的线程，并根据调度算法做出决策。
- 上下文恢复：调度器选择了下一个要执行的线程后，它会从该线程保存的上下文信息中恢复线程的执行状态。
- 切换到新线程：调度器将执行权切换到新线程，使其开始执行。

上下文信息的保存通常由操作系统负责管理，具体保存在哪里取决于操作系统的实现方式。一般情况下，上下文信息会保存在线程的控制块（Thread Control Block，TCB）中。

TCB是操作系统用于管理线程的数据结构，包含了线程的状态、寄存器的值、堆栈信息等。当发生线程切换时，操作系统会通过切换TCB来保存和恢复线程的上下文信息。



## 守护进程，僵尸进程，孤儿进程？

**守护进程（Daemon Process）**： 守护进程是一种在后台运行的特殊进程，通常用于提供某种服务或执行定期任务。守护进程没有控制终端（Controlling Terminal），因此不会与用户交互。它们通常在系统启动时启动，并在系统关闭时终止。守护进程的名称通常以 `d` 结尾，如 `sshd`（Secure Shell Daemon）、`httpd`（HTTP Daemon）等。要创建守护进程，通常需要执行以下操作：

1. 调用 `fork()` 产生子进程，然后让父进程退出。这样，子进程会成为孤儿进程，被 init 进程（进程ID为1）收养，从而摆脱原始的控制终端。
2. 调用 `setsid()` 创建新的会话（Session）并成为会话组长，以确保进程不再拥有控制终端。
3. 改变当前工作目录（例如，切换到根目录）。
4. 重设文件权限掩码（umask）。
5. 关闭不需要的文件描述符。
6. 处理相关信号（如 SIGHUP、SIGTERM 等）。

**僵尸进程（Zombie Process）**： 僵尸进程是一种已经终止但仍占用进程表（Process Table）空间的进程。当一个进程终止时，其子进程的状态会变为僵尸进程，直到父进程通过调用 `wait()` 或 `waitpid()` 系统调用回收其资源。僵尸进程不再占用 CPU 或内存资源，但会占用进程表空间。如果系统产生大量僵尸进程，可能导致进程表耗尽，从而影响系统性能。为避免僵尸进程，父进程应当及时回收已终止子进程的资源。

> **一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵尸进程。**

**孤儿进程（Orphan Process）**： 孤儿进程是指父进程在子进程之前终止，导致子进程失去父进程的情况。在 Unix 和类 Unix 系统中，孤儿进程会被 init 进程（进程ID为1）收养。init 进程会定期调用 `wait()` 或 `waitpid()` 系统调用，以回收孤儿进程的资源。因此，孤儿进程不会成为僵尸进程。虽然孤儿进程可能会在一段时间内无人管理，但它们最终会被 init 进程收养并得到妥善处理。孤儿进程仍然可以独立运行，完成其任务，直到它们自然结束或被操作系统终止。

**总结**

- 守护进程：后台运行的特殊进程，用于提供服务或执行定期任务，没有控制终端。
- 僵尸进程：已经终止但仍占用进程表空间的进程，需要父进程调用 `wait()` 或 `waitpid()` 回收资源。
- 孤儿进程：父进程在子进程之前终止的进程，会被 init 进程收养并最终得到妥善处理。

### 总结

1. **孤儿进程：**父进程先结束**【爹先挂了】，**被**init养父(进程号为1)** 收养**,**并被重新设置为其子进程
2. **僵尸进程：**子进程终止，但父进程没有使用wait或waitpid收集其资源**【爹不管】**
3. **守护进程：**在后台运行，不与任何终端关联的进程**【后台天使】**

**三者的区别：**

- 孤儿进程父进程先结束，守护进程不依赖父进程
- 僵尸进程已终止但留存PCB，守护进程是持续运行
- 守护进程脱离终端后台运行，其他两种与终端无关



### **僵尸进程怎样产生的：**

一个进程在调用exit命令结束自己的生命的时候，其实它并没有真正的被销毁，而是留下一个称为僵尸进程（Zombie）的数据结构（系统调用 exit，它的作用是使进程退出，但也仅仅限于将一个正常的进程变成一个僵尸进程，并不能将其完全销毁）。
在Linux进程的状态中，僵尸进程是非常特殊的一种，它已经放弃了几乎所有内存空间，没有任何可执行代码，也不能被调度，仅仅在进程列表中保留一个位置，记载该进程的退出状态等信息供其他进程收集，除此之外，僵尸进程不再占有任何内存空间。它需要它的父进程来为它收尸，如果他的父进程没安装 SIGCHLD信号处理函数调用wait或waitpid()等待子进程结束，又没有显式忽略该信号，那么它就一直保持僵尸状态，如果这时父进程结束了， 那么init进程自动会接手这个子进程，为它收尸，它还是能被清除的。**但是如果父进程是一个循环，不会结束，那么子进程就会一直保持僵尸状态，这就是 为什么系统中有时会有很多的僵尸进程。**

### **怎么查看僵尸进程**

利用命令：**ps**，可以看到有标记为**Z**的进程就是僵尸进程。

### **怎么清除僵尸进程**

方法一： **改写父进程，在子进程死后要为它收尸。**
具体做法是接管**SIGCHLD**信号。子进程死后，会发送SIGCHLD信号给父进程，父进程收到此信号后，执行waitpid()函数为子进程收尸。这是基于这样的原理：就算父进程没有调用 wait，内核也会向它发送SIGCHLD消息，尽管对的默认处理是忽略，如果想响应这个消息，可以设置一个处理函数。

方法二：
把父进程杀掉。父进程死后，僵尸进程成为"孤儿进程"，过继给进程init，init始终会负责清理僵尸进程。它产生的所有僵尸进程也跟着消失。
**注：僵尸进程将会导致资源浪费，而孤儿则不会。**



## Java线程：内核线程还是用户线程？

在Java中，线程是并发编程的基本单位。了解Java线程的本质对于理解其行为和性能特征至关重要。本文将首先明确定义内核线程和用户线程，然后介绍内核态与用户态线程的区别，并分析为什么Java线程是**内核线程**。

### 内核线程与用户线程的定义

内核线程（Kernel-level Thread KLT）是由操作系统内核直接管理和调度的线程。它们依赖于**操作系统的调度策略**，并且可以直接访问系统资源。它们存在于内核的地址空间中，拥有自己的线程栈、线程状态等信息，并且内核线程之间的切换是由操作系统内核完成的。

**用户线程**（User-level Thread）又称为轻量级进程（Lightweight Process, LWP）它们完全在用户空间中实现，不直接由内核管理。用户线程之间的切换不需要内核的参与，而是由线程库在用户空间内完成，因此切换速度非常快。然而，当用户线程需要进行系统调用或阻塞操作时，需要映射到内核线程上执行。

### 内核态与用户态线程的区别

1. **管理方式**：内核线程由操作系统内核直接管理，而用户线程则是由线程库在用户空间内管理。
2. **切换开销**：内核线程的切换涉及到操作系统内核的介入，因此切换开销相对较大。而用户线程的切换在用户空间内完成，不需要内核的参与，因此切换速度非常快。
3. **资源占用**：内核线程由于直接由内核管理，所以每个内核线程都需要占用一定的内核资源。而用户线程则不直接占用内核资源，资源占用较少。
4. **系统调用与阻塞操作**：当用户线程需要进行系统调用或阻塞操作时，需要映射到内核线程上执行，这会增加一定的复杂性和开销
5. **资源访问**：内核线程可以直接访问系统资源，例如文件、网络套接字等。用户线程则需要通过系统调用来请求这些资源，这会带来额外的开销。

### Java线程

Java线程实际上是基于内核线程实现的。这是因为Java的设计目标之一是提供跨平台的一致性和可移植性，而用户线程在不同操作系统上的行为可能有所不同。通过使用内核线程，Java确保了线程的行为在所有支持的平台上是一致的。

此外，Java的早期版本曾尝试使用绿色线程（用户线程），但由于一些限制和问题（例如无法充分利用多核处理器），后来的版本放弃了这种实现方式。现在，Java线程直接映射到底层操作系统的内核线程上，这样做有以下好处：

- **更好的性能**：通过利用内核线程，Java线程可以利用操作系统的优化和调度策略，从而提高性能。
- **简化模型**：Java程序员无需关心底层的线程实现细节，可以专注于编写高效的并发代码。
- **平台兼容性**：虽然Java是跨平台的，但线程的行为在所有平台上保持一致，这有助于确保应用程序的可移植性。

### JAVA虚拟线程

java的线程本身其实是由JVM来管理的，java线程与内核线程是1对1的关系，即一个java线程对应一个内核线程，会导致大量的线程上下文切换。

在JDK19的版本中，java终于推出了自己的虚拟线程技术，在JDK21中正式发布虚拟线程。其实就是多个java线程对应1个或者多个内核线程。

java虚拟线程的优点：

1. **轻量级与高效性**：虚拟线程是轻量级的，这意味着它们可以比传统线程创建更多的数量，并且开销要少得多。这使得在自己的线程中运行单独任务或请求变得更加实用，即使在高吞吐量的程序中也是如此。
2. **简化并发编程**：使用虚拟线程，Java开发者可以更容易地编写并发程序，而无需处理复杂的线程管理和同步问题。每个虚拟线程可以独立运行，而不需要担心线程切换和调度的开销。
3. **提高可扩展性**：由于虚拟线程是轻量级的，因此可以创建数百万个虚拟线程，而不会像传统线程那样受到资源限制。这使得Java应用程序能够更好地处理大量并发任务，从而提高了系统的可扩展性。
4. **减少线程调度的开销**：虚拟线程的思想是在操作系统线程的基础上增加一个轻量级并发调度对象。当原有操作系统线程发生阻塞时，可以调度运行同属于一个操作系统线程的另外一个虚拟线程，这种方式比传统的线程切换更加高效。
5. **提高应用程序吞吐量**：在并发任务数量很高的情况下，虚拟线程可以显著提高应用程序的吞吐量。这尤其适用于包含大量并发任务的传统服务器应用程序，这些任务通常花费大量时间等待。
6. **增强可观测性**：虚拟线程还增强了代码的可观测性，使开发人员能够更好地调试代码，这对于故障排除、维护和优化非常重要。

> 作者：辣条小号
> 链接：https://www.jianshu.com/p/11edca4e9223
> 来源：简书
> 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



## 虚拟内存是什么？有什么作用？

如果没有虚拟内存，程序读写的地址是物理地址的话，可能会出现物理地址冲突的问题，比如，第一个程序在 2000 的位置写入一个新的值，将会擦掉第二个程序存放在相同位置上的所有内容。

为了解决这个问题，就引出了虚拟内存，操作系统为每个进程分配独立的一套「**虚拟地址**」，人人都有，大家自己玩自己的地址就行，互不干涉。但是有个前提每个进程都不能访问物理地址，至于虚拟地址最终怎么落到物理内存里，对进程来说是透明的，操作系统已经把这些都安排的明明白白了。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/J0g14CUwaZfao8qtpmKRfSDiaXDJQjw7icETicicgEWxAvkqiaDnTwVh6sfD0PHoa9Vibg4wYk45Q8ptiaFeNbjWjaQRQ/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

**操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。**

如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运行的时候，写入的是不同的物理地址，这样就不会冲突了。

最后，说下虚拟内存有什么作用？

- 第一，虚拟内存可以使得进程对运行内存超过物理内存大小，因为程序运行符合局部性原理，CPU 访问内存会有很明显的重复访问的倾向性，对于那些没有被经常使用到的内存，我们可以把它换出到物理内存之外，比如硬盘上的 swap 区域。
- 第二，由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的，这就解决了多进程之间地址冲突的问题。



## 如何避免预读失效和缓存污染的问题？

传统的 LRU 算法法无法避免下面这两个问题：

- 预读失效导致缓存命中率下降；
- 缓存污染导致缓存命中率下降；

为了避免「预读失效」造成的影响，Linux 和 MySQL 对传统的 LRU 链表做了改进：

- Linux 操作系统实现两个了 LRU 链表：**活跃 LRU 链表（active list）和非活跃 LRU 链表（inactive list）**。
- MySQL Innodb 存储引擎是在一个 LRU 链表上划分来 2 个区域：**young 区域 和 old 区域**。

但是如果还是使用「只要数据被访问一次，就将数据加入到活跃 LRU 链表头部（或者 young 区域）」这种方式的话，那么**还存在缓存污染的问题**。

为了避免「缓存污染」造成的影响，Linux 操作系统和 MySQL Innodb 存储引擎分别提高了升级为热点数据的门槛：

- Linux 操作系统：在内存页被访问**第二次**的时候，才将页从 inactive list 升级到 active list 里。

- MySQL Innodb：在内存页被访问第二次的时候，并不会马上将该页从 old 区域升级到 young 区域，因为还要进行

  停留在 old 区域的时间判断：

  - 如果第二次的访问时间与第一次访问的时间**在 1 秒内**（默认值），那么该页就**不会**被从 old 区域升级到 young 区域；
- 如果第二次的访问时间与第一次访问的时间**超过 1 秒**，那么该页就**会**从 old 区域升级到 young 区域；

通过提高了进入 active list （或者 young 区域）的门槛后，就很好了避免缓存污染带来的影响。



## malloc 是如何分配内存的？

> [4.2 malloc 是如何分配内存的？ | 小林coding](https://xiaolincoding.com/os/3_memory/malloc.html)

### [#](https://xiaolincoding.com/os/3_memory/malloc.html#linux-进程的内存分布长什么样)Linux 进程的内存分布长什么样？

在 Linux 操作系统中，虚拟地址空间的内部又被分为**内核空间和用户空间**两部分，不同位数的系统，地址空间的范围也不同。比如最常见的 32 位和 64 位系统，如下所示：

<img src="https://cdn.xiaolincoding.com//mysql/other/1db038e1d2e5325b05e2bb80475d962a.png" alt="图片" style="zoom: 67%;" />

通过这里可以看出：

- `32` 位系统的内核空间占用 `1G`，位于最高处，剩下的 `3G` 是用户空间；
- `64` 位系统的内核空间和用户空间都是 `128T`，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。

再来说说，内核空间与用户空间的区别：

- 进程在用户态时，只能访问用户空间内存；
- 只有进入内核态后，才可以访问内核空间的内存；

虽然每个进程都各自有独立的虚拟内存，但是**每个虚拟内存中的内核地址，其实关联的都是相同的物理内存**。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。

<img src="https://cdn.xiaolincoding.com//mysql/other/c88bda5db60029f3ea57e4306e7da936.png" alt="图片" style="zoom:67%;" />

接下来，进一步了解虚拟空间的划分情况，用户空间和内核空间划分的方式是不同的，内核空间的分布情况就不多说了。

我们看看用户空间分布的情况，以 32 位系统为例，我画了一张图来表示它们的关系：

通过这张图你可以看到，用户空间内存从**低到高**分别是 6 种不同的内存段：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98/32%E4%BD%8D%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%B8%83%E5%B1%80.png" alt="虚拟内存空间划分" style="zoom: 50%;" />

- 代码段，包括二进制可执行代码；
- 数据段，包括已初始化的静态常量和全局变量；
- BSS 段，包括未初始化的静态变量和全局变量；
- 堆段，包括动态分配的内存，从低地址开始向上增长；
- 文件映射段，包括动态库、共享内存等，从低地址开始向上增长（[跟硬件和内核版本有关 (opens new window)](http://lishiwen4.github.io/linux/linux-process-memory-location)）；
- 栈段，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 `8 MB`。当然系统也提供了参数，以便我们自定义大小；

在这 6 个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 `malloc()` 或者 `mmap()` ，就可以分别在堆和文件映射段动态分配内存。

### [#](https://xiaolincoding.com/os/3_memory/malloc.html#malloc-是如何分配内存的)malloc 是如何分配内存的？

实际上，malloc() 并不是系统调用，而是 C 库里的函数，用于动态分配内存。

malloc 申请内存的时候，会有两种方式向操作系统申请堆内存。

- 方式一：通过 brk() 系统调用从堆分配内存
- 方式二：通过 mmap() 系统调用在文件映射区域分配内存；

方式一实现的方式很简单，就是通过 brk() 函数将「堆顶」指针向高地址移动，获得新的内存空间。如下图：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/brk%E7%94%B3%E8%AF%B7.png" alt="img" style="zoom: 50%;" />

方式二通过 mmap() 系统调用中「私有匿名映射」的方式，在文件映射区分配一块内存，也就是从文件映射区“偷”了一块内存。如下图：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/mmap%E7%94%B3%E8%AF%B7.png" alt="img" style="zoom: 50%;" />

> 什么场景下 malloc() 会通过 brk() 分配内存？又是什么场景下通过 mmap() 分配内存？

malloc() 源码里默认定义了一个阈值：

- 如果用户分配的内存小于 128 KB，则通过 brk() 申请内存；
- 如果用户分配的内存大于 128 KB，则通过 mmap() 申请内存；

注意，不同的 glibc 版本定义的阈值也是不同的。

### [#](https://xiaolincoding.com/os/3_memory/malloc.html#malloc-分配的是物理内存吗)malloc() 分配的是物理内存吗？

不是的，**malloc() 分配的是虚拟内存**。

如果分配后的虚拟内存没有被访问的话，虚拟内存是不会映射到物理内存的，这样就不会占用物理内存了。

只有在访问已分配的虚拟地址空间的时候，操作系统通过查找页表，发现虚拟内存对应的页没有在物理内存中，就会触发缺页中断，然后操作系统会建立虚拟内存和物理内存之间的映射关系。

### [#](https://xiaolincoding.com/os/3_memory/malloc.html#malloc-1-会分配多大的虚拟内存)malloc(1) 会分配多大的虚拟内存？

malloc() 在分配内存的时候，并不是老老实实按用户预期申请的字节数来分配内存空间大小，而是**会预分配更大的空间作为内存池**。

具体会预分配多大的空间，跟 malloc 使用的内存管理器有关系，我们就以 malloc 默认的内存管理器（Ptmalloc2）来分析。

接下里，我们做个实验，用下面这个代码，通过 malloc 申请 1 字节的内存时，看看操作系统实际分配了多大的内存空间。

```c
#include <stdio.h>
#include <malloc.h>

int main() {
  printf("使用cat /proc/%d/maps查看内存分配\n",getpid());
  
  //申请1字节的内存
  void *addr = malloc(1);
  printf("此1字节的内存起始地址：%x\n", addr);
  printf("使用cat /proc/%d/maps查看内存分配\n",getpid());
 
  //将程序阻塞，当输入任意字符时才往下执行
  getchar();

  //释放内存
  free(addr);
  printf("释放了1字节的内存，但heap堆并不会释放\n");
  
  getchar();
  return 0;
}
```

执行代码（**先提前说明，我使用的 glibc 库的版本是 2.17**）：

<img src="https://cdn.xiaolincoding.com//mysql/other/080ee187c8c92db45092b6688774e8da.png" alt="图片" style="zoom:67%;" />

我们可以通过 /proc//maps 文件查看进程的内存分布情况。我在 maps 文件通过此 1 字节的内存起始地址过滤出了内存地址的范围。

```shell
[root@xiaolin ~]# cat /proc/3191/maps | grep d730
00d73000-00d94000 rw-p 00000000 00:00 0                                  [heap]
```

这个例子分配的内存小于 128 KB，所以是通过 brk() 系统调用向堆空间申请的内存，因此可以看到最右边有 [heap] 的标识。

可以看到，堆空间的内存地址范围是 00d73000-00d94000，这个范围大小是 132KB，也就说明了 **malloc(1) 实际上预分配 132K 字节的内存**。

可能有的同学注意到了，程序里打印的内存起始地址是 `d73010`，而 maps 文件显示堆内存空间的起始地址是 `d73000`，为什么会多出来 `0x10` （16字节）呢？这个问题，我们先放着，后面会说。

### [#](https://xiaolincoding.com/os/3_memory/malloc.html#free-释放内存-会归还给操作系统吗)free 释放内存，会归还给操作系统吗？

我们在上面的进程往下执行，看看通过 free() 函数释放内存后，堆内存还在吗？

<img src="https://cdn.xiaolincoding.com//mysql/other/1a9337f8f6b83fbc186f257511b5ce67.png" alt="图片" style="zoom:67%;" />

从下图可以看到，通过 free 释放内存后，堆内存还是存在的，并没有归还给操作系统。

![图片](https://cdn.xiaolincoding.com//mysql/other/2b8f63892830553ec04c5f05f336ae8b.png)

这是因为与其把这 1 字节释放给操作系统，不如先缓存着放进 malloc 的内存池里，当进程再次申请 1 字节的内存时就可以直接复用，这样速度快了很多。

当然，当进程退出后，操作系统就会回收进程的所有资源。

上面说的 free 内存后堆内存还存在，是针对 malloc 通过 brk() 方式申请的内存的情况。

如果 malloc 通过 mmap 方式申请的内存，free 释放内存后就会归归还给操作系统。

我们做个实验验证下， 通过 malloc 申请 128 KB 字节的内存，来使得 malloc 通过 mmap 方式来分配内存。

```c
#include <stdio.h>
#include <malloc.h>

int main() {
  //申请1字节的内存
  void *addr = malloc(128*1024);
  printf("此128KB字节的内存起始地址：%x\n", addr);
  printf("使用cat /proc/%d/maps查看内存分配\n",getpid());

  //将程序阻塞，当输入任意字符时才往下执行
  getchar();

  //释放内存
  free(addr);
  printf("释放了128KB字节的内存，内存也归还给了操作系统\n");

  getchar();
  return 0;
}
```

执行代码：

<img src="https://cdn.xiaolincoding.com//mysql/other/500fdc021d956f60963f308760f511d0.png" alt="图片" style="zoom:67%;" />

查看进程的内存的分布情况，可以发现最右边没有 [heap] 标志，说明是通过 mmap 以匿名映射的方式从文件映射区分配的匿名内存。

![图片](https://cdn.xiaolincoding.com//mysql/other/501f458b8d35abe5e378a0f14c667797.png)

然后我们释放掉这个内存看看：

<img src="https://cdn.xiaolincoding.com//mysql/other/fcdbe91cc03b6a2f6e93dd1971d1b438.png" alt="图片" style="zoom:67%;" />

再次查看该 128 KB 内存的起始地址，可以发现已经不存在了，说明归还给了操作系统。

<img src="https://cdn.xiaolincoding.com//mysql/other/3f63c56b131d92806b5aabca29d33a38.png" alt="图片" style="zoom:67%;" />

对于 「malloc 申请的内存，free 释放内存会归还给操作系统吗？」这个问题，我们可以做个总结了：

- malloc 通过 **brk()** 方式申请的内存，free 释放内存的时候，**并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用**；
- malloc 通过 **mmap()** 方式申请的内存，free 释放内存的时候，**会把内存归还给操作系统，内存得到真正的释放**。

### [#](https://xiaolincoding.com/os/3_memory/malloc.html#为什么不全部使用-mmap-来分配内存)为什么不全部使用 mmap 来分配内存？

因为向操作系统申请内存，是要通过系统调用的，执行系统调用是要进入内核态的，然后在回到用户态，运行态的切换会耗费不少时间。

所以，申请内存的操作应该避免频繁的系统调用，如果都用 mmap 来分配内存，等于每次都要执行系统调用。

另外，因为 mmap 分配的内存每次释放的时候，都会归还给操作系统，于是每次 mmap 分配的虚拟地址都是缺页状态的，然后在第一次访问该虚拟地址的时候，就会触发缺页中断。

也就是说，**频繁通过 mmap 分配的内存话，不仅每次都会发生运行态的切换，还会发生缺页中断（在第一次访问虚拟地址后），这样会导致 CPU 消耗较大**。

为了改进这两个问题，malloc 通过 brk() 系统调用在堆空间申请内存的时候，由于堆空间是连续的，所以直接预分配更大的内存来作为内存池，当内存释放的时候，就缓存在内存池中。

**等下次在申请内存的时候，就直接从内存池取出对应的内存块就行了，而且可能这个内存块的虚拟地址与物理地址的映射关系还存在，这样不仅减少了系统调用的次数，也减少了缺页中断的次数，这将大大降低 CPU 的消耗**。

### [#](https://xiaolincoding.com/os/3_memory/malloc.html#既然-brk-那么牛逼-为什么不全部使用-brk-来分配)既然 brk 那么牛逼，为什么不全部使用 brk 来分配？

前面我们提到通过 brk 从堆空间分配的内存，并不会归还给操作系统，那么我们那考虑这样一个场景。

如果我们连续申请了 10k，20k，30k 这三片内存，如果 10k 和 20k 这两片释放了，变为了空闲内存空间，如果下次申请的内存小于 30k，那么就可以重用这个空闲内存空间。

<img src="https://cdn.xiaolincoding.com//mysql/other/75edee0cb75450e7987a8a482b975bda.png" alt="图片" style="zoom: 67%;" />

但是如果下次申请的内存大于 30k，没有可用的空闲内存空间，必须向 OS 申请，实际使用内存继续增大。

因此，随着系统频繁地 malloc 和 free ，尤其对于小块内存，堆内将产生越来越多不可用的碎片，导致“内存泄露”。而这种“泄露”现象使用 valgrind 是无法检测出来的。

所以，malloc 实现中，充分考虑了 brk 和 mmap 行为上的差异及优缺点，默认分配大块内存 (128KB) 才使用 mmap 分配内存空间。

### [#](https://xiaolincoding.com/os/3_memory/malloc.html#free-函数只传入一个内存地址-为什么能知道要释放多大的内存)free() 函数只传入一个内存地址，为什么能知道要释放多大的内存？

还记得，我前面提到， malloc 返回给用户态的内存起始地址比进程的堆空间起始地址多了 16 字节吗？

这个多出来的 16 字节就是保存了该内存块的描述信息，比如有该内存块的大小。

![图片](https://cdn.xiaolincoding.com//mysql/other/cb6e3ce4532ff0a6bfd60fe3e52a806e.png)

这样当执行 free() 函数时，free 会对传入进来的内存地址向左偏移 16 字节，然后从这个 16 字节的分析出当前的内存块的大小，自然就知道要释放多大的内存了。



## 软连接和硬连接有什么区别？

**软连接**实际上是一个指向目标文件的路径的符号链接，类似于Windows系统中的快捷方式，创建软连接不会占用目标文件的inode节点，只是简单地指向目标文件的路径。删除原始文件后，软连接仍然存在，但指向的目标文件失效，称为"悬空链接"。软链接可以跨文件系统创建软连接。

**硬连接**是指多个文件实际上指向同一个inode节点，即多个文件共享同一块数据块。创建硬连接会增加目标文件的链接计数，删除任何一个硬连接并不会影响其他硬连接指向的文件数据。只能在同一文件系统内创建硬连接。



### 什么是硬链接与符号连接？

硬链接（hard link）和符号链接（symbolic link，也称为软链接）是Unix和类Unix文件系统中两种不同的文件链接类型。它们用于创建文件或目录的引用。

**硬链接**

硬链接是文件系统中一个文件的额外引用。在Unix和类Unix文件系统中，每个文件都有一个称为inode的数据结构来存储文件的元数据，例如文件权限、所有者、大小等。每个文件都有一个或多个文件名（硬链接），它们指向相应的inode。换句话说，硬链接是文件名和inode之间的关联。

硬链接的特点如下：

- 硬链接不能跨文件系统。由于硬链接直接关联到inode，它只能在同一个文件系统中创建。
- 硬链接不能引用目录。这是为了防止文件系统中出现循环引用和其他不一致性问题。
- 删除一个文件的所有硬链接会导致文件被删除。当一个文件的最后一个硬链接被删除时，文件系统将释放该文件的inode以及占用的存储空间。
- 硬链接不影响原始文件的访问。所有硬链接都指向相同的inode，因此访问任何一个硬链接实际上是访问原始文件。

**符号链接**

符号链接是一种特殊的文件，它包含指向另一个文件或目录的路径。与硬链接直接关联到inode不同，符号链接通过路径名来引用目标文件。当用户或应用程序访问符号链接时，文件系统会自动将其重定向到目标路径。

符号链接的特点如下：

- 符号链接可以跨文件系统。由于符号链接通过路径名引用目标文件，它可以链接到其他文件系统中的文件或目录。
- 符号链接可以引用目录。这使得符号链接在文件系统组织和目录结构管理中非常有用。
- 删除符号链接不会影响目标文件。当删除一个符号链接时，只有链接本身被删除，而目标文件保持不变。
- 符号链接可能引起死链接（dangling link）。如果目标文件被删除或移动，符号链接将指向一个不存在的路径，导致死链接。



## 锁

> **总结：**
>
> 开发过程中，最常见的就是互斥锁的了，互斥锁加锁失败时，会用「线程切换」来应对，当加锁失败的线程再次加锁成功后的这一过程，会有两次线程上下文切换的成本，性能损耗比较大。
>
> 如果我们明确知道被锁住的代码的执行时间很短，那我们应该选择开销比较小的自旋锁，因为自旋锁加锁失败时，并不会主动产生线程切换，而是一直忙等待，直到获取到锁，那么如果被锁住的代码执行时间很短，那这个忙等待的时间相对应也很短。
>
> 如果能区分读操作和写操作的场景，那读写锁就更合适了，它允许多个读线程可以同时持有读锁，提高了读的并发性。根据偏袒读方还是写方，可以分为读优先锁和写优先锁，读优先锁并发性很强，但是写线程会被饿死，而写优先锁会优先服务写线程，读线程也可能会被饿死，那为了避免饥饿的问题，于是就有了公平读写锁，它是用队列把请求锁的线程排队，并保证先入先出的原则来对线程加锁，这样便保证了某种线程不会被饿死，通用性也更好点。
>
> 互斥锁和自旋锁都是最基本的锁，读写锁可以根据场景来选择这两种锁其中的一个进行实现。
>
> 另外，互斥锁、自旋锁、读写锁都属于悲观锁，悲观锁认为并发访问共享资源时，冲突概率可能非常高，所以在访问共享资源前，都需要先加锁。
>
> 相反的，如果并发访问共享资源时，冲突概率非常低的话，就可以使用乐观锁，它的工作方式是，在访问共享资源时，不用先加锁，修改完共享资源后，再验证这段时间内有没有发生冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作。
>
> 但是，一旦冲突概率上升，就不适合使用乐观锁了，因为它解决冲突的重试成本非常高。
>
> 不管使用的哪种锁，我们的加锁的代码范围应该尽可能的小，也就是加锁的粒度要小，这样执行速度会比较快。再来，使用上了合适的锁，就会快上加快了。

### 什么是互斥锁，自旋锁呢，底层是怎么实现的？

互斥锁（Mutex）和自旋锁（Spinlock）是两种用于同步和保护共享资源的锁机制，它们都可以防止多个线程或进程同时访问共享资源，从而避免竞态条件（Race Condition）和数据不一致等问题。

**互斥锁（Mutex）**： 互斥锁是一种用于实现线程或进程间同步的机制。当一个线程获得互斥锁并访问共享资源时，其他试图获得该锁的线程将被阻塞，直到锁被释放。互斥锁可以保证同一时刻只有一个线程能够访问共享资源。互斥锁的底层实现通常依赖于操作系统的原语，例如在Linux系统中使用pthread库的pthread_mutex_t数据结构来实现互斥锁。

**自旋锁（Spinlock）**： 自旋锁是一种低级的同步原语，通常用于多处理器或多核系统中。与互斥锁不同，当一个线程尝试获得自旋锁时，如果锁已经被其他线程持有，它将不断循环（“自旋”）检查锁是否可用，而不是进入阻塞状态。自旋锁适用于锁持有时间较短且线程不希望在等待锁时进入睡眠状态的场景。自旋锁的底层实现通常依赖于原子操作和CPU指令，如测试和设置（test-and-set）或比较和交换（compare-and-swap）等。

互斥锁和自旋锁的主要区别在于它们在等待锁时的行为：

- 当线程尝试获得已被占用的互斥锁时，它会进入阻塞状态，让出CPU资源，等待锁被释放。
- 当线程尝试获得已被占用的自旋锁时，它会不断循环检查锁是否可用，而不会让出CPU资源。

> 上下切换的耗时有大佬统计过，大概在几十纳秒到几微秒之间，如果你锁住的代码执行时间比较短，那可能上下文切换的时间都比你锁住的代码执行时间还要长。
>
> 所以，**如果你能确定被锁住的代码执行时间很短，就不应该用互斥锁，而应该选用自旋锁，否则使用互斥锁。**



### 什么是读写锁？

读写锁（Read-Write Lock）是一种用于同步访问共享资源的锁机制，适用于读操作远多于写操作的场景。与互斥锁（Mutex）不同，读写锁允许多个线程同时进行读操作，但在进行写操作时，只允许一个线程访问共享资源。这种锁机制可以提高多线程程序的性能，因为它允许多个线程在不互相干扰的情况下进行并发读操作。

读写锁具有以下特性：

1. 共享读：多个线程可以同时获得读锁，共享读取共享资源。这意味着在没有写操作的情况下，读操作不会被阻塞。
2. 独占写：当一个线程获得写锁时，其他线程无法获得读锁或写锁。这可以确保在写操作期间，共享资源不会被其他线程修改或访问。
3. 优先级：实现读写锁时，可能需要处理读写操作之间的优先级。根据实现方式的不同，读写锁可能会更倾向于优先处理读操作，或者在某些情况下，优先处理写操作。这可能会导致写饥饿或读饥饿的问题，需要根据实际场景进行权衡。

读写锁在实现时通常依赖于底层的操作系统原语。例如，在Linux系统中，可以使用pthread库中的pthread_rwlock_t数据结构来实现读写锁。

总之，读写锁是一种适用于读操作远多于写操作场景的同步机制。通过允许多个线程同时进行读操作，读写锁可以提高多线程程序在访问共享资源时的性能。然而，根据实现方式的不同，读写锁可能需要处理读写操作之间的优先级问题。



### 乐观锁与悲观锁

前面提到的互斥锁、自旋锁、读写锁，都是属于悲观锁。

悲观锁做事比较悲观，它认为**多线程同时修改共享资源的概率比较高，于是很容易出现冲突，所以访问共享资源前，先要上锁**。

那相反的，如果多线程同时修改共享资源的概率比较低，就可以采用乐观锁。

乐观锁做事比较乐观，它假定冲突的概率很低，它的工作方式是：**先修改完共享资源，再验证这段时间内有没有发生冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作**。

放弃后如何重试，这跟业务场景息息相关，虽然重试的成本很高，但是冲突的概率足够低的话，还是可以接受的。

可见，乐观锁的心态是，不管三七二十一，先改了资源再说。另外，你会发现**乐观锁全程并没有加锁，所以它也叫无锁编程**。

这里举一个场景例子：在线文档。

我们都知道在线文档可以同时多人编辑的，如果使用了悲观锁，那么只要有一个用户正在编辑文档，此时其他用户就无法打开相同的文档了，这用户体验当然不好了。

那实现多人同时编辑，实际上是用了乐观锁，它允许多个用户打开同一个文档进行编辑，编辑完提交之后才验证修改的内容是否有冲突。

怎么样才算发生冲突？这里举个例子，比如用户 A 先在浏览器编辑文档，之后用户 B 在浏览器也打开了相同的文档进行编辑，但是用户 B 比用户 A 提交早，这一过程用户 A 是不知道的，当 A 提交修改完的内容时，那么 A 和 B 之间并行修改的地方就会发生冲突。

服务端要怎么验证是否冲突了呢？通常方案如下：

- 由于发生冲突的概率比较低，所以先让用户编辑文档，但是浏览器在下载文档时会记录下服务端返回的文档版本号；
- 当用户提交修改时，发给服务端的请求会带上原始文档版本号，服务器收到后将它与当前版本号进行比较，如果版本号不一致则提交失败，如果版本号一致则修改成功，然后服务端版本号更新到最新的版本号。

实际上，我们常见的 SVN 和 Git 也是用了乐观锁的思想，先让用户编辑代码，然后提交的时候，通过版本号来判断是否产生了冲突，发生了冲突的地方，需要我们自己修改后，再重新提交。

乐观锁虽然去除了加锁解锁的操作，但是一旦发生冲突，重试的成本非常高，所以**只有在冲突概率非常低，且加锁成本非常高的场景时，才考虑使用乐观锁。**



## 死锁条件是什么？

死锁只有**同时满足**以下四个条件才会发生：

- 互斥条件：是指**多个线程不能同时使用同一个资源**。
- 持有并等待条件：指当线程 A 已经持有了资源 1，又想申请资源 2，而资源 2 已经被线程 C 持有了，所以线程 A 就会处于等待状态，但是**线程 A 在等待资源 2 的同时并不会释放自己已经持有的资源 1**。
- 不可剥夺条件：指当线程已经持有了资源 ，**在自己使用完之前不能被其他线程获取**，线程 B 如果也想使用此资源，则只能在线程 A 使用完并释放后才能获取。
- 环路等待条件：指的是在死锁发生的时候，**两个线程获取资源的顺序构成了环形链**。



## **处理死锁**

分为预防、避免和检测恢复三种策略：

**预防死锁**： 预防死锁的方法是破坏死锁产生的四个条件中的一个或多个。例如：

- **破坏占有且等待条件：**要求线程/进程在请求资源之前释放所有已经持有的资源，或者一次性请求所有需要的资源。
- **破坏非抢占条件**：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。
- **破坏循环等待条件：**为所有资源分配一个全局的顺序，并要求线程/进程按照这个顺序请求资源。

**避免死锁**： 避免死锁的方法是在运行时动态地检查资源分配情况，以确保系统不会进入不安全状态。银行家算法是一种著名的避免死锁的算法，通过模拟资源分配过程来判断是否会产生死锁，如果会产生死锁，则拒绝分配资源。

**检测和恢复死锁**： 检测和恢复死锁的方法是允许系统进入死锁状态，然后定期检测死锁，并在发现死锁后采取措施解决。常见的检测方法包括资源分配图（Resource Allocation Graph）和检测算法。恢复死锁的方法通常包括以下几种：

- 终止线程/进程：强制终止一个或多个死锁中的线程/进程，从而释放其持有的资源。这种方法可能会导致数据丢失或不一致，因此需要谨慎使用。
- 回滚线程/进程：将死锁中的线程/进程回滚到之前的某个状态，然后重新执行。这种方法需要系统支持事务和恢复功能，并且可能会影响系统性能。
- 动态资源分配：在检测到死锁后，尝试动态地分配资源，以解除死锁。例如，可以向系统请求更多资源，或者在不影响整体性能的情况下调整资源分配策略。
- 等待和重试：在某些情况下，可以让死锁中的线程/进程等待一段时间，以便其他线程/进程释放资源。等待一段时间后，重新尝试请求资源。这种方法可能会导致线程/进程长时间处于等待状态，从而影响系统性能。





## 信号量是如何实现的？

信号量（Semaphore）是一种同步原语，用于实现多线程和多进程之间的同步和互斥。信号量的本质是一个整数计数器，通常用于限制对共享资源的访问数量。信号量的实现涉及到两个关键操作：wait（或称为P操作）和post（或称为V操作）。

**基本实现原理**

1. **初始化**：信号量在创建时需要进行初始化，通常将计数器设置为允许同时访问共享资源的最大数量。
2. **Wait（P）操作**：当一个线程或进程想要访问共享资源时，会执行wait操作。在wait操作中，信号量的计数器减1。如果计数器的值为负数，表示没有可用的资源，执行wait操作的线程/进程将被阻塞，直到有资源可用。
3. **Post（V）操作**：当一个线程或进程完成对共享资源的访问后，会执行post操作。在post操作中，信号量的计数器加1。如果计数器的值小于等于0，表示有等待的线程/进程，此时会唤醒一个被阻塞的线程/进程。

信号量的实现依赖于底层操作系统原语，以保证wait和post操作的原子性。在Linux系统中，信号量有两种实现方式：System V信号量和POSIX信号量。

1. **System V信号量**：System V信号量使用一组系统调用（如semget、semop和semctl）实现。这些系统调用提供了原子性操作，以保证信号量的正确性。System V信号量具有更强的跨进程特性，可以在不相关的进程之间使用。
2. **POSIX信号量**：POSIX信号量使用sem_t数据结构，并通过一组函数（如sem_init、sem_wait、sem_trywait、sem_post和sem_destroy）提供信号量操作。POSIX信号量在现代Linux系统中较为常用，因为它们具有较好的可移植性和性能。



## 条件变量是如何实现的？

条件变量（Condition Variable）是一种用于实现线程间同步的原语。条件变量允许线程等待某个条件的满足，当条件满足时，其他线程会通知等待的线程。条件变量通常与互斥锁（Mutex）一起使用，以保护共享资源的访问和同步。

**初始化**：条件变量在创建时需要进行初始化。在Linux中，使用pthread_cond_t数据结构表示条件变量，并通过pthread_cond_init函数进行初始化。

**等待条件**：当一个线程需要等待某个条件满足时，它会执行以下操作：

1. 首先，线程获取互斥锁，保护共享资源的访问。
2. 然后，线程检查条件是否满足。如果条件不满足，线程会调用pthread_cond_wait或pthread_cond_timedwait函数，将自己阻塞并等待条件变量的通知。在进入阻塞状态之前，pthread_cond_wait函数会自动释放关联的互斥锁，以允许其他线程访问共享资源。
3. 当条件变量收到通知时，阻塞在条件变量上的线程会被唤醒。pthread_cond_wait函数返回时，会自动重新获取关联的互斥锁。
4. 唤醒后，线程需要重新检查条件是否满足，因为可能存在虚假唤醒（Spurious Wakeup）的情况。如果条件满足，线程继续执行；否则，线程将继续等待条件变量的通知。

**通知条件**：当另一个线程改变了共享资源，并使得条件满足时，它需要执行以下操作：

1. 首先，线程获取互斥锁，保护共享资源的访问。
2. 然后，线程修改共享资源，使得条件满足。
3. 接着，线程调用pthread_cond_signal或pthread_cond_broadcast函数，通知等待条件变量的一个或所有线程。
4. 最后，线程释放互斥锁，允许其他线程访问共享资源。

在Linux中，条件变量的实现依赖于底层操作系统原语，以保证线程间的同步和通知操作的原子性。



## [#](https://www.xiaolincoding.com/os/5_schedule/schedule.html#_6-1-进程调度-页面置换-磁盘调度算法)进程调度/页面置换/磁盘调度算法



## [#](https://www.xiaolincoding.com/os/8_network_system/zero_copy.html#_9-1-什么是零拷贝)9.1 什么是零拷贝？



## I/O 多路复用：select/poll/epoll

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%E6%8F%90%E7%BA%B2.png)

> **总结：**
>
> 最基础的 TCP 的 Socket 编程，它是阻塞 I/O 模型，基本上只能一对一通信，那为了服务更多的客户端，我们需要改进网络 I/O 模型。
>
> 比较传统的方式是使用多进程/线程模型，每来一个客户端连接，就分配一个进程/线程，然后后续的读写都在对应的进程/线程，这种方式处理 100 个客户端没问题，但是当客户端增大到 10000 个时，10000 个进程/线程的调度、上下文切换以及它们占用的内存，都会成为瓶颈。
>
> 为了解决上面这个问题，就出现了 I/O 的多路复用，可以只在一个进程里处理多个文件的 I/O，Linux 下有三种提供 I/O 多路复用的 API，分别是：select、poll、epoll。
>
> select 和 poll 并没有本质区别，它们内部都是使用「线性结构」来存储进程关注的 Socket 集合。
>
> 在使用的时候，首先需要把关注的 Socket 集合通过 select/poll 系统调用从用户态拷贝到内核态，然后由内核检测事件，当有网络事件产生时，内核需要遍历进程关注 Socket 集合，找到对应的 Socket，并设置其状态为可读/可写，然后把整个 Socket 集合从内核态拷贝到用户态，用户态还要继续遍历整个 Socket 集合找到可读/可写的 Socket，然后对其处理。
>
> 很明显发现，select 和 poll 的缺陷在于，当客户端越多，也就是 Socket 集合越大，Socket 集合的遍历和拷贝会带来很大的开销，因此也很难应对 C10K。
>
> epoll 是解决 C10K 问题的利器，通过两个方面解决了 select/poll 的问题。
>
> - epoll 在内核里使用「红黑树」来关注进程所有待检测的 Socket，红黑树是个高效的数据结构，增删改一般时间复杂度是 O(logn)，通过对这棵黑红树的管理，不需要像 select/poll 在每次操作时都传入整个 Socket 集合，减少了内核和用户空间大量的数据拷贝和内存分配。
> - epoll 使用事件驱动的机制，内核里维护了一个「链表」来记录就绪事件，只将有事件发生的 Socket 集合传递给应用程序，不需要像 select/poll 那样轮询扫描整个集合（包含有和无事件的 Socket ），大大提高了检测的效率。
>
> 而且，epoll 支持边缘触发和水平触发的方式，而 select/poll 只支持水平触发，一般而言，边缘触发的方式会比水平触发的效率高。



### 最基本的 Socket 模型

要想客户端和服务器能在网络中通信，那必须得使用 Socket 编程，它是进程间通信里比较特别的方式，特别之处在于它是可以跨主机间通信。

创建 Socket 的时候，可以指定网络层使用的是 IPv4 还是 IPv6，传输层使用的是 TCP 还是 UDP。

UDP 的 Socket 编程相对简单些，这里我们只介绍基于 TCP 的 Socket 编程。

服务器的程序要先跑起来，然后等待客户端的连接和数据，我们先来看看服务端的 Socket 编程过程是怎样的。

服务端首先调用 `socket()` 函数，创建网络协议为 IPv4，以及传输协议为 TCP 的 Socket ，接着调用 `bind()` 函数，给这个 Socket 绑定一个 **IP 地址和端口**，绑定这两个的目的是什么？

- 绑定端口的目的：当内核收到 TCP 报文，通过 TCP 头里面的端口号，来找到我们的应用程序，然后把数据传递给我们。
- 绑定 IP 地址的目的：一台机器是可以有多个网卡的，每个网卡都有对应的 IP 地址，当绑定一个网卡时，内核在收到该网卡上的包，才会发给我们；

绑定完 IP 地址和端口后，就可以调用 `listen()` 函数进行监听，此时对应 TCP 状态图中的 `listen`，如果我们要判定服务器中一个网络程序有没有启动，可以通过 `netstat` 命令查看对应的端口号是否有被监听。

服务端进入了监听状态后，通过调用 `accept()` 函数，来从内核获取客户端的连接，如果没有客户端连接，则会阻塞等待客户端连接的到来。

那客户端是怎么发起连接的呢？客户端在创建好 Socket 后，调用 `connect()` 函数发起连接，该函数的参数要指明服务端的 IP 地址和端口号，然后万众期待的 TCP 三次握手就开始了。

在 TCP 连接的过程中，服务器的内核实际上为每个 Socket 维护了两个队列：

- 一个是「还没完全建立」连接的队列，称为 **TCP 半连接队列**，这个队列都是没有完成三次握手的连接，此时服务端处于 `syn_rcvd` 的状态；
- 一个是「已经建立」连接的队列，称为 **TCP 全连接队列**，这个队列都是完成了三次握手的连接，此时服务端处于 `established` 状态；

当 TCP 全连接队列不为空后，服务端的 `accept()` 函数，就会从内核中的 TCP 全连接队列里拿出一个已经完成连接的 Socket 返回应用程序，后续数据传输都用这个 Socket。

注意，监听的 Socket 和真正用来传数据的 Socket 是两个：

- 一个叫作**监听 Socket**；
- 一个叫作**已连接 Socket**；

连接建立后，客户端和服务端就开始相互传输数据了，双方都可以通过 `read()` 和 `write()` 函数来读写数据。

至此， TCP 协议的 Socket 程序的调用过程就结束了，整个过程如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/tcp_socket.png)

### 如何服务更多的用户？

前面提到的 TCP Socket 调用流程是最简单、最基本的，它基本只能一对一通信，因为使用的是同步阻塞的方式，当服务端在还没处理完一个客户端的网络 I/O 时，或者 读写操作发生阻塞时，其他客户端是无法与服务端连接的。

可如果我们服务器只能服务一个客户，那这样就太浪费资源了，于是我们要改进这个网络 I/O 模型，以支持更多的客户端。

在改进网络 I/O 模型前，我先来提一个问题，你知道服务器单机理论最大能连接多少个客户端？

相信你知道 TCP 连接是由四元组唯一确认的，这个四元组就是：**本机IP, 本机端口, 对端IP, 对端端口**。

服务器作为服务方，通常会在本地固定监听一个端口，等待客户端的连接。因此服务器的本地 IP 和端口是固定的，于是对于服务端 TCP 连接的四元组只有对端 IP 和端口是会变化的，所以**最大 TCP 连接数 = 客户端 IP 数×客户端端口数**。

对于 IPv4，客户端的 IP 数最多为 2 的 32 次方，客户端的端口数最多为 2 的 16 次方，也就是**服务端单机最大 TCP 连接数约为 2 的 48 次方**。

这个理论值相当“丰满”，但是服务器肯定承载不了那么大的连接数，主要会受两个方面的限制：

- **文件描述符**，Socket 实际上是一个文件，也就会对应一个文件描述符。在 Linux 下，单个进程打开的文件描述符数是有限制的，没有经过修改的值一般都是 1024，不过我们可以通过 ulimit 增大文件描述符的数目；
- **系统内存**，每个 TCP 连接在内核中都有对应的数据结构，意味着每个连接都是会占用一定内存的；

那如果服务器的内存只有 2 GB，网卡是千兆的，能支持并发 1 万请求吗？

并发 1 万请求，也就是经典的 C10K 问题 ，C 是 Client 单词首字母缩写，C10K 就是单机同时处理 1 万个请求的问题。

从硬件资源角度看，对于 2GB 内存千兆网卡的服务器，如果每个请求处理占用不到 200KB 的内存和 100Kbit 的网络带宽就可以满足并发 1 万个请求。

不过，要想真正实现 C10K 的服务器，要考虑的地方在于服务器的网络 I/O 模型，效率低的模型，会加重系统开销，从而会离 C10K 的目标越来越远。



### select/poll

select 实现多路复用的方式是，将已连接的 Socket 都放到一个**文件描述符集合**，然后调用 select 函数将文件描述符集合**拷贝**到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过**遍历**文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合**拷贝**回用户态里，然后用户态还需要再通过**遍历**的方法找到可读或可写的 Socket，然后再对其处理。

所以，对于 select 这种方式，需要进行 **2 次「遍历」文件描述符集合**，一次是在内核态里，一个次是在用户态里 ，而且还会发生 **2 次「拷贝」文件描述符集合**，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。

select 使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 `1024`，只能监听 0~1023 的文件描述符。

poll 不再用 BitsMap 来存储所关注的文件描述符，取而代之用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制。

但是 poll 和 select 并没有太大的本质区别，**都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合**，这种方式随着并发数上来，性能的损耗会呈指数级增长。

### epoll

先复习下 epoll 的用法。如下的代码中，先用epoll_create 创建一个 epoll对象 epfd，再通过 epoll_ctl 将需要监视的 socket 添加到epfd中，最后调用 epoll_wait 等待数据。

```c
int s = socket(AF_INET, SOCK_STREAM, 0);
bind(s, ...);
listen(s, ...)

int epfd = epoll_create(...);
epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中

while(1) {
    int n = epoll_wait(...);
    for(接收到数据的socket){
        //处理
    }
}
```

epoll 通过两个方面，很好解决了 select/poll 的问题。

*第一点*，epoll 在内核里使用**红黑树来跟踪进程所有待检测的文件描述字**，把需要监控的 socket 通过 `epoll_ctl()` 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删改一般时间复杂度是 `O(logn)`。而 select/poll 内核里没有类似 epoll 红黑树这种保存所有待检测的 socket 的数据结构，所以 select/poll 每次操作时都传入整个 socket 集合给内核，而 epoll 因为在内核维护了红黑树，可以保存所有待检测的 socket ，所以只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配。

*第二点*， epoll 使用**事件驱动**的机制，内核里**维护了一个链表来记录就绪事件**，当某个 socket 有事件发生时，通过**回调函数**内核会将其加入到这个就绪事件列表中，当用户调用 `epoll_wait()` 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。

从下图你可以看到 epoll 相关的接口作用：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/epoll.png)

epoll 的方式即使监听的 Socket 数量越多的时候，效率不会大幅度降低，能够同时监听的 Socket 的数目也非常的多了，上限就为系统定义的进程打开的最大文件描述符个数。因而，**epoll 被称为解决 C10K 问题的利器**。



### select poll epoll的区别与联系？

`select`, `poll`和`epoll`都是I/O多路复用技术，它们用于同时处理多个I/O操作，特别是在高并发网络编程中。

**select**

`select`是最早的I/O多路复用技术，它可以同时监视多个文件描述符（file descriptor, FD）的I/O状态（如可读、可写、异常等）。`select`函数使用一个文件描述符集合（通常是一个位图）来表示要监视的文件描述符，当有I/O事件发生时，`select`会返回对应的文件描述符集合。

select的主要限制如下：

- 文件描述符数量限制：`select`使用一个位图来表示文件描述符集合，这限制了它能够处理的文件描述符数量（通常是1024个）。
- 效率问题：当文件描述符数量较大时，`select`需要遍历整个文件描述符集合来查找就绪的文件描述符，这会导致较低的效率。
- 非实时性：每次调用`select`时，需要重新设置文件描述符集合，这会增加函数调用的开销。

**poll**

`poll`是为了克服`select`的限制而引入的一种I/O多路复用技术。`poll`使用一个文件描述符数组（通常是一个结构体数组）来表示要监视的文件描述符。与`select`类似，`poll`可以监视多个文件描述符的I/O状态。

poll的优点如下：

- 文件描述符数量不受限制：由于`poll`使用一个动态数组来表示文件描述符，因此它可以处理任意数量的文件描述符。
- 效率相对较高：`poll`在查找就绪的文件描述符时，只需要遍历实际使用的文件描述符数组，而不是整个文件描述符集合。

然而，`poll`仍然存在一些问题：

- 效率问题：尽管`poll`相对于`select`具有较高的效率，但当文件描述符数量很大时，它仍然需要遍历整个文件描述符数组。
- 非实时性：与`select`类似，每次调用`poll`时，需要重新设置文件描述符数组。

**epoll**

`epoll`是Linux特有的一种高效I/O多路复用技术，它克服了`select`和`poll`的主要限制。`epoll`使用一个事件驱动（event-driven）的方式来处理I/O操作，它只会返回就绪的文件描述符，而不是遍历整个文件描述符集合。

epoll的主要优点如下：

- 高效：`epoll`使用事件驱动的方式来处理I/O操作，因此它在处理大量文件描述符时具有很高的效率。当有I/O事件发生时，`epoll`可以立即得到通知，而无需遍历整个文件描述符集合。这使得`epoll`在高并发场景中具有更好的性能。
- 可扩展性：与`poll`类似，`epoll`可以处理任意数量的文件描述符，因为它使用一个动态数据结构来表示文件描述符。
- 实时性：`epoll`使用一个内核事件表来记录要监视的文件描述符和事件，因此在每次调用`epoll`时无需重新设置文件描述符集合。这可以减少函数调用的开销，并提高实时性。

`epoll`具有诸多优点，但它目前仅在Linux平台上可用。对于其他平台，可能需要使用类似的I/O多路复用技术，如BSD中的`kqueue`。

**总结**：`select`是最早的I/O多路复用技术，但受到文件描述符数量和效率方面的限制。`poll`克服了文件描述符数量的限制，但仍然存在一定的效率问题。`epoll`是一种高效的I/O多路复用技术，尤其适用于高并发场景，但它仅在Linux平台上可用。一般来说，epoll的效率是要比select和poll高的，但是对于活动连接较多的时候，由于回调函数触发的很频繁，其效率不一定比select和poll高。所以epoll在连接数量很多，但活动连接较小的情况性能体现的比较明显。

| 系统调用             | select                                                       | poll                                                         | epoll                                                        |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 事件集合             | 用户通过三个参数分别传入感兴趣的可读可写以及异常等事件，内核通过对这些参数的在线修改来反馈其中就绪的事件。如果用户需要的话需要创建三个fdset以监听不同类型的事件。 | 统一处理所有的事件类型，因此只需要一个事件集参数。用户通过pollfd.events来传入感兴趣的事件，内核通过修改pollfd.revents反馈其中就绪的事件。 | 内核通过一个事件表直接管理用户感兴趣的所有事件。每次调用epoll_wait，内核直接在调用参数的events中注册就绪事件。 |
| 应用程序索引效率     | 采用轮询方式，O(n)                                           | 采用轮询方式，O(n)                                           | 采用回调方式，O(1)                                           |
| 最大支持文件描述符数 | 一般1024                                                     | 65535                                                        | 65535                                                        |
| 工作模式             | 条件触发                                                     | 条件触发                                                     | 条件触发或边缘触发                                           |



### 边缘触发与条件触发分别是什么？

边缘触发（Edge-triggered）和条件触发（Level-triggered）是两种常见的事件触发方式，主要应用于I/O多路复用和中断处理等场景。

**边缘触发（Edge-triggered）**

边缘触发是指在事件状态发生变化的时刻触发一次，例如从无事件变为有事件。在I/O多路复用中，边缘触发意味着当某个文件描述符发生I/O事件（如变为可读或可写）时，我们只会收到一次通知。当收到通知后，我们需要处理该文件描述符上的所有数据，直到数据全部处理完毕，否则不会再收到通知。

边缘触发的优点是只在事件状态改变时触发，可以减少事件通知的次数。然而，边缘触发的缺点是我们需要确保在收到通知后处理所有相关数据，否则可能会遗漏某些事件。

**条件触发（Level-triggered）**

条件触发是指只要事件状态保持满足某种条件，就会持续触发。在I/O多路复用中，条件触发意味着只要某个文件描述符的I/O事件状态满足条件（如可读或可写），我们就会不断收到通知。

条件触发的优点是它可以确保我们不会遗漏任何事件，因为只要条件满足，就会持续触发。然而，条件触发的缺点是它可能导致大量的事件通知，从而增加处理开销。







## CPU Cache 的数据写入

CPU 在读写数据的时候，都是在 CPU Cache 读写数据的，原因是 Cache 离 CPU 很近，读写性能相比内存高出很多。对于 Cache 里没有缓存 CPU 所需要读取的数据的这种情况，CPU 则会从内存读取数据，并将数据缓存到 Cache 里面，最后 CPU 再从 Cache 读取数据。

而对于数据的写入，CPU 都会先写入到 Cache 里面，然后再在找个合适的时机写入到内存，那就有「写直达」和「写回」这两种策略来保证 Cache 与内存的数据一致性：

- 写直达，只要有数据写入，都会直接把数据写入到内存里面，这种方式简单直观，但是性能就会受限于内存的访问速度；
- 写回，对于已经缓存在 Cache 的数据的写入，只需要更新其数据就可以，不用写入到内存，只有在需要把缓存里面的脏数据交换出去的时候，才把数据同步到内存里，这种方式在缓存命中率高的情况，性能会更好；



## 讲一讲Inode文件系统结构？

Inode（索引节点）是UNIX和类UNIX文件系统中的一个重要概念。Inode是文件系统中的一个数据结构，用于存储文件或目录的元数据（metadata），例如文件大小、时间戳、权限、所有者等。Inode还包含了指向实际文件数据块（data block）的指针，这些数据块存储了文件的内容。

在UNIX和类UNIX文件系统中，每个文件或目录都有一个唯一的Inode号（Inode number），用于唯一标识文件系统中的对象。文件名只是一个用户友好的引用，实际上是指向Inode号的指针。这种设计允许文件系统以更高效的方式管理和访问文件，同时提供了硬链接等高级功能。以下是Inode文件系统结构的几个关键组成部分：

- **Inode表**：Inode表是文件系统中一个预先分配的区域，用于存储Inode数据结构。每个Inode在Inode表中占用固定大小的空间，通常为128字节或256字节。文件系统在格式化时会预先分配一定数量的Inode，这些Inode数量决定了文件系统能够容纳的最大文件和目录数量。
- **数据块**：数据块（data block）是文件系统中用于存储文件内容的基本单位。数据块的大小通常为4KB、8KB或更大。每个Inode包含了指向文件数据块的指针，这些指针可以直接指向数据块（直接块指针），也可以指向间接块（间接块指针）、二次间接块（双间接块指针）或三次间接块（三间接块指针）。
- **目录项**：目录项（directory entry）是文件系统中表示目录结构的数据结构。每个目录项包含一个文件名和一个对应的Inode号。目录项存储在目录文件的数据块中，通过Inode号可以找到目录项所指向的文件或子目录的Inode和数据块。
- **超级块**：超级块（superblock）是文件系统中存储文件系统元数据的区域。超级块包含了文件系统的基本信息，如文件系统类型、大小、块大小、Inode数量等。操作系统在挂载文件系统时会读取超级块，以确定文件系统的参数和状态。

![image-20230410154719558](https://www.csview.cn/assets/image-20230410154719558-2764e473.png)



[7.1 文件系统全家桶 | 小林coding (xiaolincoding.com)](https://xiaolincoding.com/os/6_file_system/file_system.html#目录的存储)

![image-20240904105807943](assets\image-20240904105807943.png)

![image-20240904105819458](assets\image-20240904105819458.png)